% This is part of the Avaneya Project Crew Handbook.
% Copyright (C) 2010, 2011, 2012
%   Kshatra Corp.
% See the file License for copying conditions.

% Algorithms, Optimizations, & Tips section...
\StartSection{Algorithms, Optimizations, & Tips}
We will share in this section some of the different algorithms, optimizations, and tips that are useful in tackling the many different problems the project will address at some point. Consider the following advice tentative, without thought of absolute commitment, and hardly sacrosanct. This is because no one knows with certainty what the future holds and we may find down the road any one of the following approaches problematic, prompting a need to explore alternatives. 

With every bit of eye candy, bell, and whistle we add to any software, there is the inherent problem of an increased burden on the hardware. Thus, we need to take advantage of optimizations whenever possible. 

As a general rule, it is more important to be concerned with asymptotic complexity, or how your algorithm scales, than getting bogged down in constant time optimizations at specific stages of execution that only offers modest improvement at best. But assessing an algorithm's complexity can be daunting. Fortunately you do not need a degree from a computer science department to understand the concept,\footnote{\href{http://www.eternallyconfuzzled.com/arts/jsw_art_bigo.aspx}{Asymptotic Notation. }{\it Julienne Walker}. 1 Apr. 2012.} despite the poor approach common in its teaching. The concept is very practical, there to help you, and need not be a black art of the esoteric.

Nevertheless, sometimes having already taken an algorithm's quadratic performance to linear logarithmic, or whatever the situation may be, useful constant time optimizations may be all that remain to consider. Most libraries the AresEngine links against, such as SDL and OGRE 3D, are probably already optimized for whatever architecture their compilation is supported on. But of course not necessarily everything an engine consists of, as well as the scripts that drive it, are found in 3\high{rd} party libraries. If that were the case, game developers would not have anything to do.

With that in mind, what follows is some useful advice. Some of it is useful in only one specific domain, such as working with the geometry of a landscape. In other cases, it is domain agnostic for solving many different types of seemingly unrelated problem, such as refactoring an algorithm to take advantage of parallelization.

\StartSubSection{Fallbacks Shaders}
Learning tools, like Avaneya, need not be bland. We rely on visual effects, like any game, to make it interesting. Consider some creative examples. The scanline effect of a fictional terminal's interface; gas sublimation of exposed Martian ice and the ensuing dust devils; atmospheric \chemical{CO_2} clouds visible in the day sky, or glowing meteorites in the night; a construction site's flood lights and the welder's sparks; the Fresnel effect to model the amount of reflection and refraction at a fluid body's material boundary,\footnote{Rost, Randi J., and Bill Licea-Kane. {\it OpenGL Shading Language}. 3rd ed. Upper Saddle River, NJ: Addison Wesley, 2010. Print. p.404.} of chromatic aberation of ice,\footnote{{\it ibid.}, p. 409.} a jet's exhaust; or what have you, are all some of the many examples we need to implement. All of these are typically written in a high performance shader language that executes directly on the user's video card.

Those who work in the proprietary software industry sometimes over inflate the capabilities of their target user's hardware. This is because the corporations they work for are unconcerned with getting the most out of what the user already has. They profit, at least in part, when users are perpetually convinced their hardware is inadequate so as to coerce them into undue, mindless, and wasteful consumption. That is not to say that users should never upgrade, but only so long as it is actually in {\it their} interest and not merely the {\it vendor's}.

The truth is, however, that most people on this planet have never even used a telephone, let alone a top of the line, liquid cooled, \$500 graphics card. But even if everyone had powerful graphics hardware, there is still no guarantee that two users with two different vendor's top of the line cards would both have the same features set -- or even implement the ones they {\it do} share in ways that produce the {\it same} results, for that matter. 

For these reasons we need to provide fallback shaders whenever reasonable for less capable hardware, or whenever we can expect hiccups for some people. This way, users who already know their hardware is weak and do not expect it to be able to deliver what more powerful hardware can, can stil get the most out of it while those who have the latter can be satisfied knowing that it is being taken full advantage of.

But in aiming for flexibility, we must draw the line somewhere. We still require a card that at least supports a programmable shader interface. Investing time in appeasing a dead fixed function pipeline is not a useful expenditure of resources in an era where even the cheapest and most primitive graphics hardware typically supports at least some minimal of a programmable shader interface.

\StartSubSection{Instrumentation & Performance Analysis}
A general software engineering principle is to make a program work first, then to make it work better. There will come a time when the engine is sufficiently mature that some comprehensive introspection and analysis will be necessary to help identify performance bottlenecks that were difficult to anticipate during the architectural design. One approach is to use instrumentation software like GNU's {\tt gprof} and examine the derived call graphs.

On Intel architectures, we should also remember to check the machine's performance counters\footnote{{\it IntelÂ® 64 and IA--32 Architectures Optimization Reference Manual}. Intel, Nov. 2009. PDF. p. A--13.} for the frequency of cache misses. This is especially important on consoles.\footnote{Gregory, Jason. {\it Game Engine Architecture}. Wellesley, MA: K Peters, 2009. Print., p. 568.}

\StartSubSection{Graphics Memory}
It is usually a bad idea for games to make assumptions about the capabilities of a user's graphics adapter -- except when it can reliably verify them, safely provide a fallback when it cannot, and of course, do something useful with the information. 

The amount of memory a user's graphics card is equipped with can sometimes provide hints in automatically selecting, or suggesting, a certain graphics setting. This is important because a lot of users do not configure a game's graphics settings properly and they are left with a bad impression because the default settings were poor choices, albeit safely compatible with the lowest common denominator of hardware.

In the case of AMD's video cards, querying the {\tt GL_ATI_meminfo} function can provide us with this information. On nVidia cards, the following code snippet can be used instead. Note that you must link against {\tt libXNVCtrl.a} in the case of the latter.

\startCodeExample
// Determine total video memory on an nVidia graphics card...

    // Open the display...
    Display *CurrentDisplay = XOpenDisplay(NULL);

    // Perform the query and store the result in TotalMemory...
    int TotalMemory = 0;
    XNVCTRLQueryAttribute(CurrentDisplay, 0, 0, NV_CTRL_VIDEO_RAM, &TotalMemory);
    
    // Cleanup...
    XCloseDisplay(CurrentDisplay);
\stopCodeExample

\StartSubSection{Lighting Models}
For lighting, we will have to experiment with the capabilities of OGRE 3D, our rendering engine. Still, there are some algorithms to consider earmarking for the time being. For static objects, such as buildings and terrain, we may be able to take advantage of spherical harmonics for real--time lighting.\footnote{{\it ibid}. {\it OpenGL Shading Language}, p. 365.} We can also consider using deferred shading for volume shadows.\footnote{{\it ibid}. {\it OpenGL Shading Language}, p. 392.} For global or ambient illumination, we might leverage a hemispherical lighting model. Time will tell, as there is no {\it a priori} substitute for actual experimentation.

\StartSubSection{Memoization}
Memoization, not to be confused with memorization, is a technique in algorithm design that preserves the results of costly computations whenever it is possible to re--use the result without having to perform the entire calculation again for the same input. When scripting, we can take advantage of Lua's built--in {\tt memoization(f)} function whenever possible.\footnote{ Figueiredo, Luiz Henrique De., Waldemar Celes, and Roberto Ierusalimschy. {\it Lua Programming Gems}. Rio De Janeiro: Lua.org, 2008. Print. p. 26.} The function works by creating a new function that returns the same result as {\tt f} on a given input, but by memoizing its result. As long as {\tt f} does not have any side effects, we can use its memoized variant.

\StartSubSection{Path Finding}
A path finding algorithm is necessary for mobile units, such as any of the vehicles described in \in{figure}[figure:Units_User_Basic_Vehicles]. They sometimes need to self--navigate from one location to another where a non--trivial solution is necessary. By non--trivial, we mean the optimal path could involve balancing a number of different constraints, such as minimizing fuel expenditure; time of travel; time before the unit's solar array is deprived of sunlight; distance; severity of the terrain; negotiating obstacles, some of them possibly moving, unfriendly; or what have you.

For this problem we have selected the tried and true A\high{*} search strategy algorithm.\footnote{Poole, David L., Alan K. Mackworth, and Randy Goebel. {\it Computational Intelligence: A Logical Approach}. New York: Oxford UP, 1998. Print.} Most textbooks on artificial intelligence that cover search strategy algorithms include some mention of it, though varying in clarity. It is an interesting algorithm. For our implementation, take a look in \in{section}[Artificial Intelligence].

\StartSubSection{Procedurally Generated Audio}
Procedural generation is a mathematical technique for algorithmically producing some kind of data automatically, as opposed to manually. As a general rule with this project, whenever we can reasonably get away with it, as opposed to providing static data, we will. The advantages are many, but we can name a few. These include a reduced storage footprint, increased performance, a theoretical infinite resolution in some situations, and most importantly, an opportunity for every user to experience something unique that would not have been possible with static data.

In the context of graphics, procedural generation is certainly not a new concept, most graphics engineers probably having at least heard of it by now. But it is comparatively unheard of as far as creating dynamic audio in a game. 

There are many possibilities to consider. We could experiment with attempting to dynamically recreate the low-frequency rumbles of a marsquake streamed to the user's subwoofer, the sound of dry--ice hail impacting on the frozen Martian tundra, static heard over a radio, or what have you. Many books and papers have been written on the subject.\footnote{Lecky-Thompson, Guy W. {\it Infinite Game Universe: Level Design, Terrain, and Sound}. Vol. 2. Hingham, MA: Charles River Media, 2002. Print.} The possibilities are largely limited by the imagination of the engineer.

Whenever we use procedural generation for creating audio, we will need to do it all in software -- for now. There may come a time, however, when the low--level audio API we are dependent on, OpenAL, will offer a programmable shader interface for code to execute on specialized hardware that can interface directly with the user's DSP. The idea has been considered many times.\footnote{Warner, Kip. \href{http://opensource.creative.com/pipermail/openal/2010-January/011972.html}{OpenAL Shaders}. Mailing list. {\it OpenAL Usage Issues}. Creative Labs, 26 Jan. 2010. Web. 31 Mar. 2012.}

\StartSubSection{Procedurally Generated Terrain}
Terrain generation is another application. The terrain problems are common to any game that requires the rendering of a three--dimensional landscape. Height maps? Not if we want overhangs. High resolution detail at any level of magnification? Not if we want an interactive frame rate.

What about using real topographical data of Mars? Possible, but there is a problem. The best source of topographical data at the time of writing is the Mars Orbiter Laser Altimeter (MOLA) data set.\footnote{\href{http://pds-geosciences.wustl.edu/missions/mgs/mola.html}{Mars Global Surveyor: MOLA}. {\it NASA}, 21 Feb. 2012.} MOLA was one of the instruments the Mars Global Surveyor orbital spacecraft carried between the years of 1999 to 2001. Unfortunately the spatial resolution is inadequate for our purposes. For every degree of longitude at the planet's equator, there are 128 pixels of sampling available from the cylindrical projection data set. That might seem like a lot, but it is really only about half a kilometre per pixel.

This might be fine when the user is zoomed out to see an entire city, but what about if they are zoomed in really close to the landscape to interact with various objects? Even if the data's spatial resolution was fine enough to provide for elevation details at distances as small as a metre, we are now left with a new problem -- too much geometry for the machine to realistically handle at an interactive frame rate. Whatever was viewable at a micro--level, even if we hide all of the landscape that is not visible, all of the details present in what is {\it still} present in the camera's viewport will bog the machine down. But perhaps there is another approach to consider.

Ryan Geiss is well known for his non--free Geiss visualizer for Winamp. The plugin is considered a classic because its author was a pioneer in the field of sound activated graphics (visualization). But Ryan has also made contributions few have heard about in other fields, such as in procedural terrain generation. His article on {\it Generating Complex Procedural Terrains Using the GPU}\footnote{Geiss, Ryan. \href{http://http.developer.nvidia.com/GPUGems3/gpugems3_ch01.html}{Generating Complex Procedural Terrains Using the GPU}. GPU Gems 3. Upper Saddle River, NJ: Addison-Wesley, 2008. Print.} provides an attractive candidate solution for our needs. The method he describes even allows overhangs, something traditional height maps cannot handle. It also generates rich and highly detailed geometry. He manages to do this actually at an interactive frame rate by generating the geometry entirely on the fly. This is accomplished by offloading the entire computation to the GPU where he relies on fractals and several octaves of noise to produce the vertex data. Consider taking a look at one of the demonstrations he provides.\footnote{Video is 33 MB and available \href{http://www.geisswerks.com/gpugems3ch/MVI_7867.avi}{here}.}

However, we are still left to solve other problems, such as terrain deformation and satisfying the community's preference for incorporating real topographical data. In the case of the latter, it may be theoretically possible to influence Ryan's algorithm at a macro--level by \quote{seeding it} with an initial brute--force approach using what is topographically known now, while dynamically \quote{filling in} the details algorithmically on the fly.

In addition, we will probably see yet another performance improvement by harnessing hardware based OpenGL Shader Language implementations of the {\tt noise*()} functions that provide fast pseudo--random number generators our terrain shaders will need. At the time of writing, unfortunately no known hardware vendor implements these functions\footnote{Mesa 8.0, a popular software based OpenGL renderer, notes in its {\tt src/glsl/lower_noise.cpp} that {\it "no hardware has a noise instruction"}.} -- probably owing to the efforts of some parasitic patent troll. Instead, the functions usually just return {\tt 0.0} and so we must check at runtime whether they are implemented, and if not, rely on a software fallback.

\StartSubSection{Random Number Generators}
Random numbers are vital to most games. The stream of random numbers games need usually does not need to be as high quality as required in statistical and scientific computing, but clearly wanting of something better than what most standard language APIs implement. Case in point, no serious game developer actually uses their C++ compiler's {\tt std::rand()} function.

The game can query the random number stream thousands of times per second, with each number potentially decisive in forming the causal chain of events which drive the user's experience. Considering this, we selected Makoto Matsumoto's algorithm,\footnote{\href{http://www.eternallyconfuzzled.com/tuts/algorithms/jsw_tut_rand.aspx\#mersenne}{Mersenne Twister. }{\it Julienne Walker}. 1 Apr. 2012.} a popular high quality pseudo--random number generator. The algorithm offers an excellent balance between performance, quality of output, and a very large period.

As one of its many novel uses, it may be possible to parametrically define some idiosyncrasies of a unit's model by using an element of randomness. For example, two otherwise identical building units, such as a pressurized habitat, could use the random parameter to make the foliage in their indoor courtyards different from one another by each seeding a common foliage generation subroutine with a different value. The possibilities are largely limited by the technical and artistic creativity of the artists and engineers involved.

\StartSubSection{Social Simulation}
For aspects of the game concerned with social simulation, it is very difficult to commit to specific algorithms and information representation models until the establishment of more of the game's peripheral aspects. The reason being is that it is hard to tell how well an approach will perform until their is a means of trying it. Compounding the challenges further yet, of all the different subfields of game development, social simulation is probably amongst the most absent in the literature.

Something to consider at least for the time being, however, is the use of a Hubbert curve for modelling resource consumption of finite supplies of natural resources.\footnote{\href{https://en.wikipedia.org/wiki/Hubbert_curve}{Hubbert curve. }{\it Wikipedia}. 18 Mar. 2012.}

\StartSubSection{Square Roots}
Square roots are common in game programming and nearly always pop up whenever vectors, trigonometry, linear algebra, and so on, are involved. Nevertheless, avoid them whenever possible. Use the squared magnitude instead. The former is slow, the latter faster -- albeit at the cost of accuracy.

\StartSubSection{Vectorization}
Take advantage of the vectorization a given architecture's accelerated instruction set is furnished with whenever possible. This is sometimes called single instruction, multiple data (SIMD). These days, virtually every major architecture offers some variant of this concept under a name of its own. It works by taking a group of inputs and batch processing them simultaneously, as opposed to serially. For example, calculating the cross product of multiple pairs of vectors in a single instruction.

Since we are using the GNU Compiler Collection, we can leverage the already existing implementation\footnote{See the {\tt xmmintrin.h} header which ships with the GNU Compiler Collection.} of Intel's specification whenever working with {\tt amd64} or {\tt i686} architectures. This is especially useful for linear algebra and other math related routines. But probably better yet if we instead used an architecturally agnostic approach by using GCC's built--in vector extensions.\footnote
{\href{http://gcc.gnu.org/onlinedocs/gcc/Vector-Extensions.html}{Using Vector Instructions through Built-in Functions}. Using the GNU Compiler Collection (GCC). Free Software Foundation.}


